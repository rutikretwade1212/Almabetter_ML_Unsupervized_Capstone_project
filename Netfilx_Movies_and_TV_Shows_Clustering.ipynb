{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "qYpmQ266Yuh3",
        "Seke61FWphqN",
        "b0JNsNcRphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "c49ITxTc407N",
        "k5UmGsbsOxih",
        "kexQrXU-DjzY",
        "VFOzZv6IFROw",
        "-Kee-DAl2viO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV Shows clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Student Name -**   Rutik Retwade\n",
        "\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segmenting Netflix movies and TV shows is an unsupervised machine learning challenge. The initial goal is to prepare the dataset for analysis and clustering.\n",
        "\n",
        "To begin, I imported the dataset and gathered basic information about it. I then planned the data wrangling and preprocessing steps. One of the initial tasks was converting a date feature, which was originally loaded as an object, into a Pandas datetime object. This conversion makes it easier to work with date-related functions and operations.\n",
        "\n",
        "Regarding missing values, I chose not to handle them because they were sparse in certain features. These missing values could be useful for exploratory data analysis (EDA) and wouldn't significantly impact the overall analysis.\n",
        "\n",
        "The next important step was text preprocessing. To prepare the \"Description\" feature for machine learning, I performed tasks like converting text to lowercase, removing punctuation, stopping words, and eliminating extra white spaces.\n",
        "\n",
        "Once the text data was preprocessed, I used the TF-IDF vectorization technique to convert the text into numerical format. I set a maximum of 400 features to transform each review observation into a 400-dimensional feature vector.\n",
        "\n",
        "After obtaining these numerical representations, I moved on to building clustering machine learning models. I experimented with three different models: K-means, Hierarchical Clustering, and DBSCAN algorithms. Each of these methods aimed to group similar movies and TV shows together based on their characteristics."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Netflix, the largest online streaming service with over 220 million subscribers, needs to efficiently organize the shows on its platform to improve the user experience and prevent subscriber attrition.**\n",
        "\n",
        "**Creating clusters of similar shows is essential to better understand and offer personalized show recommendations to consumers based on their preferences.**\n",
        "\n",
        "**The project's objective is to group Netflix shows into distinct clusters. Shows within the same cluster are similar to one another, while those in different clusters are dissimilar.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "\n",
        "#text preprocessing libraries\n",
        "import contractions\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.base import BaseEstimator,TransformerMixin\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "\n",
        "#avoid warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD SEOLBIKE DATA SET FROM DRIVE\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Almabetter/ML Unsupervised learning project/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "G_QvBUM-iFoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Total Rows in dataset are \",df.shape[0])\n",
        "print(\"Total columns in dataset are\",df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated(keep = 'first').sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The observation above indicates that our dataset does not contain any duplicate values.**"
      ],
      "metadata": {
        "id": "0gsFjFg2kLL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Visualizing the missing values\n",
        "df.isnull().sum().plot(kind= 'bar')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel(\"Total Number of NaN values\")"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an unsupervised machine learning project, meaning we don't have a target variable. The dataset comprises 12 features and 7787 observations. Initially, it contained some duplicated entries, which I have already removed.\n",
        "\n",
        "The dataset provides details about movies and series, including various data types such as categorical, text (Description), and numerical data. Additionally, there are four features with missing values: director, cast, country, and date_added."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a description of the dataset's features:\n",
        "\n",
        "1. **show_id**: A unique identification number for each entry, which may not be particularly useful for analysis.\n",
        "2. **type**: Describes whether the entry is a series or a movie.\n",
        "3. **title**: The title of the movie or show.\n",
        "4. **director**: The name of the director responsible for the production.\n",
        "5. **cast**: Information about the cast involved in the production.\n",
        "6. **country**: The country where the movie or series is produced or belongs to.\n",
        "7. **date_added**: The date when the content was added to Netflix.\n",
        "8. **release_year**: The year when the movie or series was originally released.\n",
        "9. **rating**: The TV rating or age rating of the show.\n",
        "10. **duration**: The duration or length of the movie or series.\n",
        "11. **listed_in**: Describes the categories or genres to which the content belongs.\n",
        "12. **description**: Provides a brief description or summary of the movie or series."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "def unique_counts(df1):\n",
        "  for i in df.columns.tolist():\n",
        "    print(f\"The Unique Values of Variable ', {i}, 'are:\", df[i].unique())\n",
        "    print()\n",
        "    print('--'*50)\n",
        "    print()\n",
        "\n",
        "unique_counts(df)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#changing date_added feature into pandas datetime\n",
        "\n",
        "def handle_date_added_feature(date_added_values):\n",
        "    fin_date = []\n",
        "    for date in date_added_values:\n",
        "        if pd.isna(date):\n",
        "            fin_date.append(np.nan)\n",
        "        else:\n",
        "            #extracting day\n",
        "            day = date.split()[1]\n",
        "            day = int(day[:-1])\n",
        "            #extracting month\n",
        "            month = date.split()[0]\n",
        "            month_map = {'January':1,'February':2,'March':3,'April':4,'May':5,'June':6,'July':7,'August':8,'September':9,'October':10,'November':11,'December':12}\n",
        "            month =  month_map[month]\n",
        "            #extracting year\n",
        "            year = date.split()[-1]\n",
        "            fin_date.append(f'{year}-{month}-{day}')\n",
        "    #returning as datetime\n",
        "    return pd.to_datetime(fin_date)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['date_added'] =  handle_date_added_feature(df.date_added)"
      ],
      "metadata": {
        "id": "YdCWwUySonkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cast'] = df['cast'].apply(lambda x : np.nan if pd.isna(x) else x.split(','))"
      ],
      "metadata": {
        "id": "ilKykumMopo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is an unsupervised problem, our focus will be on conducting exploratory data analysis (EDA) using most of the available features. Our primary goal is to cluster the data based on the text features.\n",
        "\n",
        "To streamline our approach, we won't invest significant time in imputing missing values or extensive data wrangling. We've converted the `date_added` feature to a Pandas datetime format for more effective use. Additionally, we've transformed the `listed_in` and `cast` features into lists to facilitate EDA."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Number of Movies and TV Shows in the dataset\n",
        "plt.figure(figsize=(7,7))\n",
        "df.type.value_counts().plot(kind='pie',autopct='%1.2f%%')\n",
        "plt.ylabel('')\n",
        "plt.title('Movies and TV Shows in the dataset')\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart was chosen to clearly illustrate the dataset's composition by showing the percentage of movies and TV shows. It's a straightforward way to visualize the relative distribution of these categories."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are more movies (69.14%) than TV shows (30.86%) in the dataset."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the dataset can indeed help create a positive business impact for Netflix. Understanding the composition of content (movies and TV shows) allows the platform to refine its content strategy and recommendations, potentially leading to increased user engagement and retention. However, if Netflix were to ignore the preferences of the sizable TV show-watching audience, it might lead to negative growth in that user segment, causing a potential loss in subscribers and revenue. Striking a balance in content offerings remains critical to maintain overall positive business outcomes."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Top 10 directors in the dataset\n",
        "plt.figure(figsize=(10,5))\n",
        "df[~(df['director']=='Unknown')].director.value_counts().nlargest(10).plot(kind='barh')\n",
        "plt.title('Top 10 directors by number of shows directed')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart chosen, a bar plot, was selected for its effectiveness in visualizing the top 10 directors. Bar plots are particularly straightforward for comparing and interpreting data, making it easy to discern the results and identify the most prominent directors."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raul Campos and Jan Suter together have directed 18 movies / TV shows, higher than anyone in the dataset."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight from the chart is that Raul Campos and Jan Suter have directed the most movies/TV shows in the dataset (18). This information can help Netflix collaborate with successful directors for potentially better content. However, if Netflix overfocuses on a few directors, it might limit content diversity and lead to negative growth if users want more variety. Balance is key for positive business impact."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "tv_show = df[df.type == 'TV Show']\n",
        "movie = df[df.type == 'Movie']\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "tv_show.director.value_counts()[:15].plot(kind='bar',ax = ax1,title='Top 15 TV Show Directors',figsize = (20,8))\n",
        "movie.director.value_counts()[:15].plot(kind='bar',ax =ax2, title = 'Top 15 Movie Directors',figsize = (20,8))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tGY9SLOiSBsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "tv_show_cst = []\n",
        "for obs in tv_show.iterrows():\n",
        "    if type(obs[1]['cast']) is list:\n",
        "        tv_show_cst.extend(obs[1]['cast'])\n",
        "\n",
        "movie_cst = []\n",
        "for obs in movie.iterrows():\n",
        "    if type(obs[1]['cast']) is list:\n",
        "        movie_cst.extend(obs[1]['cast'])\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "pd.Series(tv_show_cst).value_counts()[:15].plot(kind='bar',ax = ax1,title='Top 15 TV Show Actors',figsize = (20,8))\n",
        "pd.Series(movie_cst).value_counts()[:15].plot(kind='bar',ax =ax2, title = 'Top 15 Movie Actors',figsize = (20,8))\n",
        "plt.show()\n",
        "\n",
        "del tv_show_cst,movie_cst"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose bar chart with subplot option in python. I chose bar chart because it helps to understand the data with categorical and numberical feature. Here our categorical data is actors name and on y-axis it is count of their movies/Tvshows in our dataset."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takahiro Sakurai acted most TV shows in our dataset with total count of 25 TV Shows. Anupam Kher is the acted most films in our Movie category with the total movie count of 32."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Chart - 3 visualization code\n",
        "\n",
        "#Checking the distribution of Movie Durations\n",
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "sns.distplot(movie['duration'].str.extract('(\\d+)'),kde=False)\n",
        "plt.title('Distplot with Normal distribution for Movies',fontweight=\"bold\")\n",
        "\n",
        "#Checking the distribution of TV SHOWS\n",
        "plt.subplot(2,1,2)\n",
        "plt.title(\"Distribution of TV Shows duration\",fontweight='bold')\n",
        "sns.countplot(x=tv_show['duration'],data=tv_show,order = tv_show['duration'].value_counts().index)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used histogram to find the distribution between movie and its duration and I used bar-graph to find the distribution between Tv_show and its duration"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most TV shows in our dataset is released in only one season almost 1600 and Most films released, has 800 minutes duration and it is normaly distributed."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "\n",
        "# Top 10 Genre in movies\n",
        "plt.figure(figsize=(18,6))\n",
        "sns.barplot(x = df[\"listed_in\"].value_counts().head(15).index,\n",
        "            y = df[\"listed_in\"].value_counts().head(15).values)\n",
        "plt.xticks(rotation=80)\n",
        "plt.title(\"Top10 Genre in Movies\",size='16',fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a bar chart with subplots in Python. The reason for choosing a bar chart is that it's great for presenting data that involves both categorical and numerical features. In this case, our categorical feature is \"Genre,\" and on the y-axis, we represent the count of movies and TV shows. This choice makes it clear and easy to comprehend how content is distributed across different genres in the dataset."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genres like Documentaries, Stand-up Comedy, Dramas, and International Movies are highly popular, and they have a significant presence in the Netflix content library. These genres often dominate the platform, indicating a strong offering of these types of content to cater to diverse viewer preferences."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# visualizing the movies and tv_shows based on the release year\n",
        "\n",
        "\n",
        "movies_year =movie['release_year'].value_counts().sort_index(ascending=False)\n",
        "tvshows_year =tv_show['release_year'].value_counts().sort_index(ascending=False)\n",
        "\n",
        "\n",
        "sns.set(font_scale=1.4)\n",
        "movies_year.plot(figsize=(12, 8), linewidth=2.5, label=\"Movies / year\")\n",
        "tvshows_year.plot(figsize=(12, 8), linewidth=2.5,label=\"TV Shows / year\")\n",
        "plt.xlabel(\"Years\", labelpad=15)\n",
        "plt.ylabel(\"Number\", labelpad=15)\n",
        "plt.title(\"Production growth yearly\", y=1.02, fontsize=22)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "line plot helps us to identify and see the trend clearly especially over time. My aim is to know the count of total number of contents became available over time."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that Netflix acheived its peak between 2017 to 2020. This may because of Corona. Because because of corona people stayed in their house which make them to spend more time in internet.So it is clearly understood by Netflix and they make sure to own more contents in that period to attract more subscribers over other OTT platform."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "#Rating based on rating system of all TV Shows\n",
        "\n",
        "tv_ratings = tv_show.groupby(['rating'])['show_id'].count().reset_index(name='count').sort_values(by='count',ascending=False)\n",
        "fig_dims = (14,7)\n",
        "fig, ax = plt.subplots(figsize=fig_dims)\n",
        "sns.pointplot(x='rating',y='count',data=tv_ratings)\n",
        "plt.title('TV Show Ratings',size='20')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "line plot helps us to find the trend among different category. Here in this we are counting the total of different movies/tv shows ratings."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TV-MA rating category content is more available in Netflix and followed by TV-14"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Count of TVshow and Movie produced in different country\n",
        "\n",
        "\n",
        "df_country = df.groupby(['country', 'type'])['show_id'].count().sort_values(ascending = False).reset_index()\n",
        "plt.figure(figsize = (15, 6))\n",
        "sns.barplot(data = df_country, x = df_country['country'][:20], y = df_country['show_id'], hue = 'type')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.ylabel('Total Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose bar chart with subplot option in python. I chose bar chart because it helps to understand the data with categorical and numberical feature. Here our categorical data is Country and on y-axis it is count of TV shows and movies"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we seen in the word cloud Netflix has more number of United States's Tv shows and Movies. Followed by India, which has highest number of movies and very low number of TV_shows comparing to the others."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HO (Null Hypothesis): Netflix has greater or equal to number of movies collectively.\n",
        "\n",
        "H1 (Alternate Hypothesis):Netflix has always less number of movies than TV-show collectively."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "import numpy as np\n",
        "\n",
        "movie_grp = df.groupby(['type',df.date_added.dt.year])['show_id'].count()['Movie']\n",
        "tv_show = df.groupby(['type',df.date_added.dt.year])['show_id'].count()['TV Show']\n",
        "\n",
        "\n",
        "\n",
        "movie_grp_mean = np.mean(movie_grp)\n",
        "tv_show_mean = np.mean(tv_show)\n",
        "\n",
        "print(\"Movie Group mean value:\",movie_grp_mean)\n",
        "print(\"TV_Show mean value:\",tv_show_mean)\n",
        "\n",
        "value_for_mean_std = np.std(movie_grp)\n",
        "overall_std = np.std(tv_show)\n",
        "\n",
        "print(\"Movie std value:\",value_for_mean_std)\n",
        "print(\"TV_Show std value:\",overall_std)\n",
        "\n",
        "ttest,pval = ttest_ind(movie_grp,tv_show)\n",
        "\n",
        "print(\"p-value\",pval)\n",
        "if pval <0.05:\n",
        "  print(\"we reject null hypothesis\")\n",
        "else:\n",
        "  print(\"we accept null hypothesis\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I did Two-sample T-test.A t-test is a type of inferential statistic which is used to determine if there is a significant difference between the means of two groups which may be related in certain features."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The One Sample t Test determines whether the sample mean is statistically different from a known or hypothesised population mean.\n",
        " The One Sample t Test is a parametric test."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HO:movies rated for kids and older kids are at least two hours long.\n",
        "\n",
        "H1:movies rated for kids and older kids are not at least two hours long."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hypothesis = df.copy(deep=True)\n",
        "df_hypothesis['duration']= df_hypothesis['duration'].str.extract('(\\d+)')\n",
        "df_hypothesis['duration'] = pd.to_numeric(df_hypothesis['duration'])\n",
        "df_hypothesis['type'] = pd.Categorical(df_hypothesis['type'], categories=['Movie','TV Show'])\n",
        "\n",
        "#group_by duration and TYPE\n",
        "group_by_= df_hypothesis[['duration','type']].groupby(by='type')"
      ],
      "metadata": {
        "id": "9nZx8oHdwY2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "movie_grp = group_by_.get_group('Movie')\n",
        "tv_show = group_by_.get_group('TV Show')\n",
        "\n",
        "\n",
        "\n",
        "movie_grp_mean = np.mean(movie_grp)\n",
        "tv_show_mean = np.mean(tv_show)\n",
        "\n",
        "print(\"Movie Group mean value:\",movie_grp_mean)\n",
        "print(\"TV_Show mean value:\",tv_show_mean)\n",
        "\n",
        "movie_grop_std = np.std(movie_grp)\n",
        "tv_show_std = np.std(tv_show)\n",
        "\n",
        "print(\"Movie std value:\",value_for_mean_std)\n",
        "print(\"TV_Show std value:\",overall_std)\n",
        "\n",
        "\n",
        "#length of groups and DOF\n",
        "n1 = len(movie_grp)\n",
        "n2= len(tv_show)\n",
        "\n",
        "\n",
        "dof = n1+n2-2\n",
        "\n",
        "sp_2 = ((n2-1)*movie_grop_std**2  + (n1-1)*tv_show_std**2) / dof\n",
        "\n",
        "sp = np.sqrt(sp_2)\n",
        "\n",
        "#tvalue\n",
        "t_val = (movie_grp_mean-tv_show_mean)/(sp * np.sqrt(1/n1 + 1/n2))\n",
        "print('tvalue',t_val[0])\n",
        "\n",
        "\n",
        "if (stats.t.ppf(0.025,dof) < t_val[0]) and (t_val[0] < stats.t.ppf(0.975,dof)):\n",
        "    print(\"We Accept the Null Hypothesis\")\n",
        "else :\n",
        "    print(\"We reject the Null Hypothesis\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I did Two-sample T-test.A t-test is a type of inferential statistic which is used to determine if there is a significant difference between the means of two groups which may be related in certain features."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The One Sample t Test determines whether the sample mean is statistically different from a known or hypothesised population mean.\n",
        " The One Sample t Test is a parametric test."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I only require the \"Description\" feature for training my machine learning model, and since there are no missing values in this feature, there's no need for imputation or handling missing data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no outliers as I am going use only Description Feature."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I going to handle only text data here so, I am not going to taking care of other variables here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#decoding non-utf-8 characters\n",
        "def remove_non_utf8_words(df,features_names):\n",
        "    df = df.copy()\n",
        "    for feature in features_names:\n",
        "        df[feature] = df[feature].apply(lambda x : x.replace('',\"'\"))\n",
        "        df[feature] = df[feature].apply(lambda x : ''.join([c for c in x if ord(c) < 128]))\n",
        "    return df"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "def expand_contractions(df,feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x : \" \".join(x.split()))\n",
        "        df[feature] = df[feature].apply(lambda x : contractions.fix(x))\n",
        "    return df"
      ],
      "metadata": {
        "id": "xyO-jNh1xRg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "def change_to_lower_case(df,feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x : x.lower())\n",
        "    return df"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuations(df,features_columns):\n",
        "    df = df.copy()\n",
        "    punctuations = string.punctuation\n",
        "    for feature in features_columns:\n",
        "        df[feature] = df[feature].apply(lambda x : x.translate(str.maketrans('','',punctuations)))\n",
        "    return df"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_urls(df,feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x :  re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x))\n",
        "    return df\n",
        "\n",
        "def remove_words_with_digits(df, feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x : \" \".join(s for s in x.split() if not any(c.isdigit() for c in s)))\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "def remove_stopwords(df,features_names):\n",
        "    df = df.copy()\n",
        "    eng_stopwords = set(stopwords.words('english'))\n",
        "    for feature in features_names:\n",
        "        df[feature] = df[feature].apply(lambda text: \" \".join(word for word in text.split() if not word in eng_stopwords))\n",
        "    return df"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize_and_normalization(df,feature_names):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x :\" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am using Lemmatization normalization because it will do better than stemming. In stemming, there is chance that it will change the word completely. But in case of Lemmatization, it is not the case, it will try to maintain the original context of the sentence."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTextDataPreprocessing(BaseEstimator,TransformerMixin):\n",
        "\n",
        "    def __init__(self,feature_names):\n",
        "        self.feature_names = feature_names\n",
        "        return None\n",
        "\n",
        "    #decoding non-utf-8 characters\n",
        "    def remove_non_utf8_words(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : x.replace('',\"'\"))\n",
        "            df[feature] = df[feature].apply(lambda x : ''.join([c for c in x if ord(c) < 128]))\n",
        "        return df\n",
        "\n",
        "    # Expand Contraction\n",
        "    def expand_contractions(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : \" \".join(x.split()))\n",
        "            df[feature] = df[feature].apply(lambda x : contractions.fix(x))\n",
        "        return df\n",
        "\n",
        "        # Lower Casing\n",
        "    def change_to_lower_case(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : x.lower())\n",
        "        return df\n",
        "\n",
        "        # Remove Punctuations\n",
        "    def remove_punctuations(self,df,features_names):\n",
        "        df = df.copy()\n",
        "        punctuations = string.punctuation\n",
        "        for feature in features_names:\n",
        "            df[feature] = df[feature].apply(lambda x : x.translate(str.maketrans('','',punctuations)))\n",
        "        return df\n",
        "\n",
        "    # Remove URLs & Remove words and digits contain digits\n",
        "    def remove_urls(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x :  re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x))\n",
        "        return df\n",
        "\n",
        "    def remove_words_with_digits(self,df, feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : \" \".join(s for s in x.split() if not any(c.isdigit() for c in s)))\n",
        "        return df\n",
        "\n",
        "    # Remove Stopwords\n",
        "    def remove_stopwords(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        eng_stopwords = set(stopwords.words('english'))\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda text: \" \".join(word for word in text.split() if not word in eng_stopwords))\n",
        "        return df\n",
        "\n",
        "    # Tokenization\n",
        "    def tokenize_and_normalization(self,df,feature_names):\n",
        "        lemmatizer=WordNetLemmatizer()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x :\" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "\n",
        "    def transform(self,df):\n",
        "        df =df.copy()\n",
        "        #removing non utf8 words\n",
        "        df = self.remove_non_utf8_words(df,self.feature_names)\n",
        "        #expanding contractions\n",
        "        df = self.expand_contractions(df,self.feature_names)\n",
        "        #changing all to lower case\n",
        "        df = self.change_to_lower_case(df,self.feature_names)\n",
        "        #remvoing punctuations\n",
        "        df = self.remove_punctuations(df,self.feature_names)\n",
        "        #removing urls\n",
        "        df = self.remove_urls(df,self.feature_names)\n",
        "        #removing words with digits\n",
        "        df = self.remove_words_with_digits(df,self.feature_names)\n",
        "        #remove stopwords\n",
        "        df = self.remove_stopwords(df,self.feature_names)\n",
        "        #remove tokenize and normalization\n",
        "        df = self.tokenize_and_normalization(df,self.feature_names)\n",
        "\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "class TextTransformer:\n",
        "    def __init__(self, feature_name, max_features=400):\n",
        "        self.feature_name = feature_name\n",
        "        self.TfidVectorizer = TfidfVectorizer(max_features=max_features)\n",
        "\n",
        "    def fit(self, df):\n",
        "        self.TfidVectorizer.fit(df[self.feature_name])\n",
        "\n",
        "    def transform(self, df):\n",
        "        df = df.copy()\n",
        "        vectors = self.TfidVectorizer.transform(df[self.feature_name]).toarray()\n",
        "        feature_names = self.TfidVectorizer.get_feature_names_out(input_features=[self.feature_name])\n",
        "        df = pd.concat([df, pd.DataFrame(vectors, columns=feature_names)], axis=1)\n",
        "        df.drop(self.feature_name, axis=1, inplace=True)\n",
        "        return df\n",
        "'''\n",
        "# Vectorizing Text\n",
        "class CustomTfidVectorizer(BaseEstimator,TransformerMixin):\n",
        "    def __init__(self,feature_name,max_features = None):\n",
        "        self.max_features = max_features\n",
        "        self.feature_name = feature_name\n",
        "        return None\n",
        "\n",
        "    def fit(self,df):\n",
        "        self.TfidVectorizer = TfidfVectorizer(max_features= self.max_features)\n",
        "        self.TfidVectorizer.fit(df[self.feature_name])\n",
        "        return self\n",
        "\n",
        "    def transform(self,df):\n",
        "        df = df.copy()\n",
        "        vectors = self.TfidVectorizer.transform(df[self.feature_name]).toarray()\n",
        "        df[self.TfidVectorizer.get_feature_names()] = vectors\n",
        "        df.drop(self.feature_name,axis = 1,inplace = True)\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "MNcv4D7VyE-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term frequency-inverse document frequency ( TF-IDF) gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document and a corpus.Not like Bag of words and Count Vector technique which treats all words equally,TF-IDF can distinguish very common words or rare words"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to use only Description feature so I am going to drop rest of the columns."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to use all the features got from vectorization."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no need to scale the data I've finally got, I've checked the data, the vectorization got is actually already in normalized form."
      ],
      "metadata": {
        "id": "FLv3mHszyZNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to use all the features which I got after vectorization to maintain the information. Dimensionality Reduction doing in the vectorization may loose its meaning of every vector."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*No* need to split the data as it is unsupervised problem, we cannot test the result of model by having the test data. so, I am going to use the whole dataset for training."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8U7ijdBuSw0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "FgU6UAUBS5mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        ""
      ],
      "metadata": {
        "id": "GonjUNI4TBiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "class TextTransformer:\n",
        "    def __init__(self, feature_name, max_features=400):\n",
        "        self.feature_name = feature_name\n",
        "        self.TfidVectorizer = TfidfVectorizer(max_features=max_features)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.TfidVectorizer.fit(X[self.feature_name])\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        vectors = self.TfidVectorizer.transform(X[self.feature_name]).toarray()\n",
        "        feature_names = self.TfidVectorizer.get_feature_names_out(input_features=[self.feature_name])\n",
        "        X = pd.concat([X, pd.DataFrame(vectors, columns=feature_names)], axis=1)\n",
        "        X.drop(self.feature_name, axis=1, inplace=True)\n",
        "        return X\n",
        "\n",
        "\n",
        "# Rest of your code\n",
        "\n",
        "text_feature_pipeline = Pipeline([\n",
        "    ('text_preprocessing', CustomTextDataPreprocessing(feature_names=['description'])),\n",
        "    ('vectorization', TextTransformer(feature_name='description', max_features=400))\n",
        "])\n",
        "description_feature_vector = text_feature_pipeline.fit_transform(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "6MpzejTqzMya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the shapes of our data\n",
        "print(\"Train data: \",description_feature_vector.shape)"
      ],
      "metadata": {
        "id": "HyIWYoXV0ZKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SEED = 10"
      ],
      "metadata": {
        "id": "S6B2bW4H4wom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "#finding optimal number of clusters using the elbow method\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Preprocess and vectorize text data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=400, stop_words='english')\n",
        "description_feature_vector = tfidf_vectorizer.fit_transform(df['description'])\n",
        "\n",
        "ssd = []\n",
        "\n",
        "# Using for loop for iterations from 2 to 14.\n",
        "for cluster in range(2, 15):\n",
        "    kmeans = KMeans(n_clusters=cluster, random_state=SEED)\n",
        "    kmeans.fit(description_feature_vector)\n",
        "    preds = kmeans.predict(description_feature_vector)\n",
        "    score = silhouette_score(description_feature_vector, preds)\n",
        "    print(\"For n_clusters = {}, Silhouette score is {}\".format(cluster, score))\n",
        "    ssd.append(kmeans.inertia_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "plt.plot(range(2, 15), ssd)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Sum of Squared distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above Elbow method and silhouette score I am choosing 12 would be the perfect number of clusters for this problem. So I am finally training kmeans with 12 clusters."
      ],
      "metadata": {
        "id": "OJ2qojUl8fyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the K-means model on a dataset\n",
        "kmeans = KMeans(n_clusters= 12, init='k-means++', random_state= SEED)\n",
        "y_predict= kmeans.fit_predict(description_feature_vector)"
      ],
      "metadata": {
        "id": "DWrf22jH8jfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict the clusters and evaluate the silhouette score\n",
        "score = silhouette_score(description_feature_vector, y_predict)\n",
        "print(\"Silhouette score is {}\".format(score))"
      ],
      "metadata": {
        "id": "HRQd8BqK8e8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reducing the number of features to visualize it in 2D or 3D plot\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Reduce the number of features using TruncatedSVD\n",
        "n_components = 3\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "X = svd.fit_transform(description_feature_vector)\n"
      ],
      "metadata": {
        "id": "gj0K26oL8tgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.title('customer segmentation based on Recency and Monetary')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_predict, s=50)\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OCJpWx-i8-lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting 3D Graph\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig=plt.figure(figsize=(15,10))\n",
        "plt.title('3d visualization of Recency Frequency and Monetary')\n",
        "ax=fig.add_subplot(111,projection='3d')\n",
        "xs=X[:,0]\n",
        "ys=X[:,1]\n",
        "zs=X[:,2]\n",
        "ax.scatter(xs,ys,zs,s=5,c = y_predict)\n",
        "ax.set_xlabel('PCA 0')\n",
        "ax.set_ylabel('PCA 1')\n",
        "ax.set_zlabel('PCA 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cRFYdzE49T_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "# Convert the sparse matrix to a dense NumPy array\n",
        "description_feature_array = description_feature_vector.toarray()\n",
        "\n",
        "plt.figure(figsize=(13, 8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(description_feature_array, method='ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Customers')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Convert the sparse matrix to a dense NumPy array\n",
        "description_feature_array = description_feature_vector.toarray()\n",
        "\n",
        "# Create and fit the AgglomerativeClustering model\n",
        "aggh = AgglomerativeClustering(n_clusters=9, affinity='euclidean', linkage='ward')\n",
        "aggh.fit(description_feature_array)\n",
        "\n",
        "# Predict using the model\n",
        "y_hc = aggh.fit_predict(description_feature_array)\n"
      ],
      "metadata": {
        "id": "AKFgOoER_Qsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Silhouette Coefficient\n",
        "print(\"Silhouette Coefficient: %0.3f\"%silhouette_score(description_feature_vector,y_hc, metric='euclidean'))"
      ],
      "metadata": {
        "id": "ZahTJKeBCDk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model 3\n",
        "\n",
        "#finding optimal number of clusters using the elbow method\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "eps_range=range(6,12) # note, we will scale this down by 100 as we want to explore 0.06 - 0.11 range\n",
        "minpts_range=range(5,14)\n",
        "\n",
        "silhouette_scores= []\n",
        "comb = []\n",
        "\n",
        "#Using for loop for iterations from 1 to 30.\n",
        "for k in eps_range:\n",
        "    for j in minpts_range:\n",
        "        # Set the model and its parameters+\n",
        "        model = DBSCAN(eps=k/100, min_samples=j)\n",
        "        # Fit the model\n",
        "        clm = model.fit(description_feature_vector)\n",
        "        # Calculate Silhoutte Score and append to a list\n",
        "        silhouette_scores.append(silhouette_score(description_feature_vector, clm.labels_, metric='euclidean'))\n",
        "        comb.append(str(k)+\"|\"+str(j)) # axis values for the graph"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the resulting Silhouette scores on a graph\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(comb, silhouette_scores, 'bo-')\n",
        "plt.xlabel('Epsilon/100 | MinPts')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score based on different combnation of Hyperparameters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FiX7-3dfDXPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = DBSCAN(eps=6/100, min_samples=5)\n",
        "clm = model.fit(description_feature_vector)\n",
        "print(silhouette_score(description_feature_vector, clm.labels_, metric='euclidean'))"
      ],
      "metadata": {
        "id": "AlH7t7QMDgGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:,0], X[:,1], c= clm.labels_)"
      ],
      "metadata": {
        "id": "lhX0bNQZDkYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette score helped us to find the best model. And helped us to choose the best model among these."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means did better job in this dataset. It gives the insight that 12 is the perfect number of cluster, but Incase of Aggloramative Clustering we saw that the 9 is the optimal cluster.Might be 9 is better, but I chose 12 in K-means, I got a better result with that using k-means. And DBSCAN doen't perform well in this dataset."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "\n",
        "filename = 'netflix_agglormative_model.sav'\n",
        "joblib.dump(aggh, filename)"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We had given the problem of unsupervised clustering using Netflix Dataset. Initially we run some basic python code snippets to understand more about the data especially its shape, no.of features, datatypes, statical information etc.. Then in EDA part we analysed the data and found :\n",
        "\n",
        "Netflix has more number of TV_shows and Movies belongs to United States.\n",
        "\n",
        "Netflix acheived its peak in owning the number of contents between 2017 to 2020.\n",
        "\n",
        "Netflix has more number of United States's Tv shows and Movies. Followed by India, which has highest number of movies and very low number of TV_shows comparing to the others.\n",
        "\n",
        "Clustering:\n",
        "\n",
        "In K-means which did better, said 12 should be the better cluster.I case of Agglomerative Clustering which too did best job in the Silhoutte score and using the dendogram I found that 9 should be the perfect cluster for this model. But finally using the Silhoutte score I concluded that K-Means did very nice job with 12 clusters.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j6eg7F8uTKvM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}